{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQw_ekfgCUP0"
      },
      "source": [
        "# Fintech final project baseline \n",
        "-----\n",
        "本 notebook 作為 fintech 金融科技導論的期末專題競賽 baseline 程式說明。\n",
        "\n",
        "* [競賽連結](https://tbrain.trendmicro.com.tw/Competitions/Details/24)\n",
        "\n",
        "首先會就資料格式以及處理說明，接續簡介模型訓練,最終預測結果並輸出目標格式。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW_SVakcZznj"
      },
      "source": [
        "Reminder: XGBoost 版本會影響 performance，請同學多注意。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "va46Qjc-n--k",
        "outputId": "a07e42db-f1a8-4474-cc1f-f09792122929"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost==1.7.1 in /home/yslin/anaconda3/envs/lab/lib/python3.8/site-packages (1.7.1)\n",
            "Requirement already satisfied: numpy in /home/yslin/anaconda3/envs/lab/lib/python3.8/site-packages (from xgboost==1.7.1) (1.23.5)\n",
            "Requirement already satisfied: scipy in /home/yslin/anaconda3/envs/lab/lib/python3.8/site-packages (from xgboost==1.7.1) (1.9.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost==1.7.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7vk_2W7kFHVB"
      },
      "outputs": [],
      "source": [
        "# import library\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import collections\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.under_sampling import TomekLinks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7J4yqaPVwJ7"
      },
      "source": [
        "## 資料前處理\n",
        "這邊針對訓練資料和測試的資料作整理。\n",
        "baseline主要會使用到的csv檔案如下:\n",
        "  - public_train_custinfo_full_hashed.csv: 包含主要要判斷的alert key對應的幾項參數，顧客id, 風險等級, 職業, 行內總資產和年齡。\n",
        "  - train_x_alert_date: 作為訓練資料的alert key以及發生日期，共23906筆。\n",
        "  - public_x_alert_date: 作為公開測試集的alert key，格式同上共1845筆。\n",
        "  - train_y_answer: 訓練資料alert key對應最後是否SAR。\n",
        "  - 預測的案件名單及提交檔案範例: 用於生成預測結果\n",
        "\n",
        "除此之外，還會使用到顧客資訊當作訓練資料:\n",
        "  - public_train_x_ccba_full_hashed.csv\n",
        "  - public_train_x_cdtx0001_full_hashed.csv\n",
        "  - public_train_x_dp_full_hashed.csv\n",
        "  - public_train_x_remit1_full_hashed.csv\n",
        "\n",
        "前處理的方式包含:\n",
        "  - 從 alert key 檢索出顧客資訊\n",
        "  - 對非數值 feature 做 label encoding\n",
        "  - 從顧客資訊中挑選適合的 features 當作訓練資料，這裡挑選離 alert date 最近的一筆顧客資訊當作 features\n",
        "  - 統計 training data 缺失值數量"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "znQLaj3_T5RY"
      },
      "outputs": [],
      "source": [
        "def preprocess(data_dir):\n",
        "    # declare csv path\n",
        "    train_alert_date_csv = os.path.join(data_dir, 'train_x_alert_date.csv')\n",
        "    cus_info_csv = os.path.join(data_dir, 'public_train_x_custinfo_full_hashed.csv')\n",
        "    y_csv = os.path.join(data_dir, 'train_y_answer.csv')\n",
        "\n",
        "    ccba_csv = os.path.join(data_dir, 'public_train_x_ccba_full_hashed.csv')\n",
        "    cdtx_csv = os.path.join(data_dir, 'public_train_x_cdtx0001_full_hashed.csv')\n",
        "    dp_csv = os.path.join(data_dir, 'public_train_x_dp_full_hashed.csv')\n",
        "    remit_csv = os.path.join(data_dir, 'public_train_x_remit1_full_hashed.csv')\n",
        "\n",
        "    public_x_csv = os.path.join(data_dir, 'public_x_alert_date.csv')\n",
        "\n",
        "    # private\n",
        "    private_cus_info_csv = os.path.join(data_dir, 'private_x_custinfo_full_hashed.csv')\n",
        "    private_ccba_csv = os.path.join(data_dir, 'private_x_ccba_full_hashed.csv')\n",
        "    private_cdtx_csv = os.path.join(data_dir, 'private_x_cdtx0001_full_hashed.csv')\n",
        "    private_dp_csv = os.path.join(data_dir, 'private_x_dp_full_hashed.csv')\n",
        "    private_remit_csv = os.path.join(data_dir, 'private_x_remit1_full_hashed.csv')\n",
        "\n",
        "    private_x_csv = os.path.join(data_dir, 'private_x_alert_date.csv')\n",
        "\n",
        "    cus_csv = [ccba_csv, cdtx_csv, dp_csv, remit_csv]\n",
        "    private_cus_csv = [private_ccba_csv, private_cdtx_csv, private_dp_csv, private_remit_csv]\n",
        "\n",
        "    date_col = ['byymm', 'date', 'tx_date', 'trans_date']\n",
        "    data_use_col = [[1,3,4,5,6,7,8,9],[2,3,4],[1,4,5,6,7,8,9,10,11],[2,3]]\n",
        "    \n",
        "    print('Reading csv...')\n",
        "    # read csv\n",
        "    df_y = pd.read_csv(y_csv)\n",
        "    df_cus_info = pd.concat([pd.read_csv(cus_info_csv), pd.read_csv(private_cus_info_csv)],axis=0, ignore_index=True)\n",
        "    df_date = pd.read_csv(train_alert_date_csv)\n",
        "    cus_data = [pd.concat([pd.read_csv(_x), pd.read_csv(_y)],axis=0, ignore_index=True) for _x, _y in zip(cus_csv, private_cus_csv)]\n",
        "    df_public_x = pd.concat([pd.read_csv(public_x_csv), pd.read_csv(private_x_csv)],axis=0, ignore_index=True)\n",
        "\n",
        "    # do label encoding\n",
        "    le = LabelEncoder()\n",
        "    cus_data[2].debit_credit = le.fit_transform(cus_data[2].debit_credit)\n",
        "\n",
        "    cnts = [0] * 4\n",
        "    labels = []\n",
        "    training_data = []\n",
        "\n",
        "    print('Start processing training data...')\n",
        "    start = time.time()\n",
        "    for i in range(df_y.shape[0]):\n",
        "        # from alert key to get customer information\n",
        "        cur_data = df_y.iloc[i]\n",
        "        alert_key, label = cur_data['alert_key'], cur_data['sar_flag']\n",
        "\n",
        "        cus_info = df_cus_info[df_cus_info['alert_key']==alert_key].iloc[0]\n",
        "        cus_id = cus_info['cust_id']\n",
        "        cus_features = cus_info.values[2:]\n",
        "\n",
        "        date = df_date[df_date['alert_key']==alert_key].iloc[0]['date']\n",
        "\n",
        "\n",
        "        cnt = 0\n",
        "        for item, df in enumerate(cus_data):\n",
        "            cus_additional_info = df[df['cust_id']==cus_id]\n",
        "            cus_additional_info = cus_additional_info[cus_additional_info[date_col[item]]<=date]\n",
        "\n",
        "            if cus_additional_info.empty:\n",
        "                cnts[item] += 1\n",
        "                len_item = len(data_use_col[item])\n",
        "                if item == 2:\n",
        "                    len_item -= 1\n",
        "                cus_features = np.concatenate((cus_features, [np.nan] * len_item), axis=0)\n",
        "            else:\n",
        "                cur_cus_feature = cus_additional_info.loc[cus_additional_info[date_col[item]].idxmax()]\n",
        "                \n",
        "                cur_cus_feature = cur_cus_feature.values[data_use_col[item]]\n",
        "                # 處理 實際金額 = 匯率*金額\n",
        "                if item == 2:\n",
        "                    cur_cus_feature = np.concatenate((cur_cus_feature[:2], [cur_cus_feature[2]*cur_cus_feature[3]], cur_cus_feature[4:]), axis=0)\n",
        "                cus_features = np.concatenate((cus_features, cur_cus_feature), axis=0)\n",
        "        labels.append(label)\n",
        "        training_data.append(cus_features)\n",
        "        print('\\r processing data {}/{}'.format(i+1, df_y.shape[0]), end = '')\n",
        "    print('Processing time: {:.3f} secs'.format(time.time()-start))\n",
        "    print('Missing value of 4 csvs:', cnts)\n",
        "\n",
        "\n",
        "    print('Start processing testing data')\n",
        "    testing_data, testing_alert_key = [], []\n",
        "    for i in range(df_public_x.shape[0]):\n",
        "        # from alert key to get customer information\n",
        "        cur_data = df_public_x.iloc[i]\n",
        "        alert_key, date = cur_data['alert_key'], cur_data['date']\n",
        "\n",
        "        cus_info = df_cus_info[df_cus_info['alert_key']==alert_key].iloc[0]\n",
        "        cus_id = cus_info['cust_id']\n",
        "        cus_features = cus_info.values[2:]\n",
        "\n",
        "        for item, df in enumerate(cus_data):\n",
        "            cus_additional_info = df[df['cust_id']==cus_id]\n",
        "            cus_additional_info = cus_additional_info[cus_additional_info[date_col[item]]<=date]\n",
        "\n",
        "            if cus_additional_info.empty:\n",
        "                len_item = len(data_use_col[item])\n",
        "                if item == 2:\n",
        "                    len_item -= 1\n",
        "                cus_features = np.concatenate((cus_features, [np.nan] * len_item), axis=0)\n",
        "            else:\n",
        "                cur_cus_feature = cus_additional_info.loc[cus_additional_info[date_col[item]].idxmax()]\n",
        "                cur_cus_feature = cur_cus_feature.values[data_use_col[item]]\n",
        "                # 處理 實際金額 = 匯率*金額\n",
        "                if item == 2:\n",
        "                    cur_cus_feature = np.concatenate((cur_cus_feature[:2], [cur_cus_feature[2]*cur_cus_feature[3]], cur_cus_feature[4:]), axis=0)\n",
        "                cus_features = np.concatenate((cus_features, cur_cus_feature), axis=0)\n",
        "\n",
        "        testing_data.append(cus_features)\n",
        "        testing_alert_key.append(alert_key)\n",
        "        # print(cus_features)\n",
        "        print('\\r processing data {}/{}'.format(i+1, df_public_x.shape[0]), end = '')\n",
        "    return np.array(training_data), labels, np.array(testing_data), testing_alert_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UCqVIdbdxWz"
      },
      "source": [
        "# 訓練資料處理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZLalEaag9L3",
        "outputId": "00254942-4b7f-4a0f-8f3b-afd5579deab7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading csv...\n",
            "Start processing training data...\n",
            " processing data 121/23906"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m data_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./data\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39m# data preprocessing\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m training_data, labels, testing_data, testing_alert_key \u001b[39m=\u001b[39m preprocess(data_dir)\n",
            "Cell \u001b[0;32mIn [4], line 61\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m     59\u001b[0m cnt \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[39mfor\u001b[39;00m item, df \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(cus_data):\n\u001b[0;32m---> 61\u001b[0m     cus_additional_info \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39;49m\u001b[39mcust_id\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m==\u001b[39;49mcus_id]\n\u001b[1;32m     62\u001b[0m     cus_additional_info \u001b[39m=\u001b[39m cus_additional_info[cus_additional_info[date_col[item]]\u001b[39m<\u001b[39m\u001b[39m=\u001b[39mdate]\n\u001b[1;32m     64\u001b[0m     \u001b[39mif\u001b[39;00m cus_additional_info\u001b[39m.\u001b[39mempty:\n",
            "File \u001b[0;32m~/anaconda3/envs/lab/lib/python3.8/site-packages/pandas/core/ops/common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m     70\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 72\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
            "File \u001b[0;32m~/anaconda3/envs/lab/lib/python3.8/site-packages/pandas/core/arraylike.py:42\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__eq__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__eq__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cmp_method(other, operator\u001b[39m.\u001b[39;49meq)\n",
            "File \u001b[0;32m~/anaconda3/envs/lab/lib/python3.8/site-packages/pandas/core/series.py:6243\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6240\u001b[0m rvalues \u001b[39m=\u001b[39m extract_array(other, extract_numpy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, extract_range\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   6242\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 6243\u001b[0m     res_values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mcomparison_op(lvalues, rvalues, op)\n\u001b[1;32m   6245\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(res_values, name\u001b[39m=\u001b[39mres_name)\n",
            "File \u001b[0;32m~/anaconda3/envs/lab/lib/python3.8/site-packages/pandas/core/ops/array_ops.py:287\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[39mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    286\u001b[0m \u001b[39melif\u001b[39;00m is_object_dtype(lvalues\u001b[39m.\u001b[39mdtype) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(rvalues, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 287\u001b[0m     res_values \u001b[39m=\u001b[39m comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n\u001b[1;32m    289\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     res_values \u001b[39m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/anaconda3/envs/lab/lib/python3.8/site-packages/pandas/core/ops/array_ops.py:75\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m     73\u001b[0m     result \u001b[39m=\u001b[39m libops\u001b[39m.\u001b[39mvec_compare(x\u001b[39m.\u001b[39mravel(), y\u001b[39m.\u001b[39mravel(), op)\n\u001b[1;32m     74\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     result \u001b[39m=\u001b[39m libops\u001b[39m.\u001b[39;49mscalar_compare(x\u001b[39m.\u001b[39;49mravel(), y, op)\n\u001b[1;32m     76\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mreshape(x\u001b[39m.\u001b[39mshape)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "data_dir = './data'\n",
        "# data preprocessing\n",
        "training_data, labels, testing_data, testing_alert_key = preprocess(data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_jqrGX0fgsV"
      },
      "source": [
        "## 缺失值補漏\n",
        "  可以發現有不少筆資料其實是有缺漏的，補上缺失值的方法有很多種，我們對於數值類資料補上中位數，對於類別類資料補上眾數。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZ4qVF1oUIss"
      },
      "outputs": [],
      "source": [
        "''' Missing Value Imputation '''\n",
        "imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
        "imp_most_frequent = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
        "# for numerical index we do imputation using median\n",
        "numerical_index = [2,4,5,6,7,8,9,10,11,14,17,24]\n",
        "# Otherwise we select the most frequent\n",
        "non_numerical_index = [0,1,3,12,13,15,16,18,19,20,21,22,23]\n",
        "\n",
        "numerical_data = training_data[:, numerical_index]\n",
        "non_numerical_data = training_data[:, non_numerical_index]\n",
        "\n",
        "imp_median.fit(numerical_data)\n",
        "numerical_data = imp_median.transform(numerical_data)\n",
        "\n",
        "imp_most_frequent.fit(non_numerical_data)\n",
        "non_numerical_data = imp_most_frequent.transform(non_numerical_data)\n",
        "\n",
        "training_data = np.concatenate((non_numerical_data, numerical_data), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM9IdAJF69-t"
      },
      "source": [
        "  此外，若類別類資料跟數字大小沒關係，我們採用 one-hot encoding 將其編碼。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmIpbiEa1WZH",
        "outputId": "7d8fc435-7535-49b5-8162-eb495e05c6f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(23906, 357)\n"
          ]
        }
      ],
      "source": [
        "# for some catogorical features, we do one hot encoding\n",
        "one_hot_index = [1,3,4,5,6,7,8,9,12]\n",
        "onehotencorder = ColumnTransformer(\n",
        "    [('one_hot_encoder', OneHotEncoder(handle_unknown='ignore'), one_hot_index)],\n",
        "    remainder='passthrough'                     \n",
        ")\n",
        "onehotencorder.fit(training_data)\n",
        "training_data = onehotencorder.transform(training_data)\n",
        "print(training_data.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 下採樣"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 17)\t1.0\n",
            "  (0, 72)\t1.0\n",
            "  (0, 93)\t1.0\n",
            "  (0, 97)\t1.0\n",
            "  (0, 99)\t1.0\n",
            "  (0, 102)\t1.0\n",
            "  (0, 125)\t1.0\n",
            "  (0, 141)\t1.0\n",
            "  (0, 336)\t1.0\n",
            "  (0, 341)\t1.0\n",
            "  (0, 342)\t4.0\n",
            "  (0, 344)\t1.0\n",
            "  (0, 345)\t375576.0\n",
            "  (0, 346)\t85428.0\n",
            "  (0, 347)\t301224.0\n",
            "  (0, 348)\t154122.0\n",
            "  (0, 352)\t151434.0\n",
            "  (0, 354)\t673.0\n",
            "  (0, 355)\t309.0\n",
            "  (0, 356)\t37421.0\n",
            "(23821, 357) (3850, 25)\n",
            "  (0, 17)\t1.0\n",
            "  (0, 72)\t1.0\n",
            "  (0, 93)\t1.0\n",
            "  (0, 97)\t1.0\n",
            "  (0, 99)\t1.0\n",
            "  (0, 102)\t1.0\n",
            "  (0, 125)\t1.0\n",
            "  (0, 141)\t1.0\n",
            "  (0, 336)\t1.0\n",
            "  (0, 341)\t1.0\n",
            "  (0, 342)\t4.0\n",
            "  (0, 344)\t1.0\n",
            "  (0, 345)\t375576.0\n",
            "  (0, 346)\t85428.0\n",
            "  (0, 347)\t301224.0\n",
            "  (0, 348)\t154122.0\n",
            "  (0, 352)\t151434.0\n",
            "  (0, 354)\t673.0\n",
            "  (0, 355)\t309.0\n",
            "  (0, 356)\t37421.0\n",
            "(23793, 357) (3850, 25)\n"
          ]
        }
      ],
      "source": [
        "# Undersampling: Tomek Links\n",
        "tl = TomekLinks()\n",
        "training_data, labels = tl.fit_resample(training_data, labels)\n",
        "print(training_data[0])\n",
        "print(training_data.shape, testing_data.shape)\n",
        "tl = TomekLinks()\n",
        "training_data, labels = tl.fit_resample(training_data, labels)\n",
        "print(training_data[0])\n",
        "print(training_data.shape, testing_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9Eb7vE1fXeZ"
      },
      "source": [
        "# XGBoost 訓練"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg-TcXydbLbj",
        "outputId": "c4c85241-7538-4393-cd4d-c73b0bd8490d"
      },
      "outputs": [],
      "source": [
        "\n",
        "import xgboost as xgb\n",
        "# 建立 XGBClassifier 模型\n",
        "xgbrModel=xgb.XGBClassifier(random_state=0)\n",
        "# 使用訓練資料訓練模型\n",
        "# xgbrModel.fit(training_data, labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.16, max_depth=30, n_estimators=200; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.16, max_depth=30, n_estimators=200; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.16, max_depth=30, n_estimators=200; total time=   1.6s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.09, max_depth=None, n_estimators=200; total time=   2.7s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.09, max_depth=None, n_estimators=200; total time=   2.7s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.09, max_depth=None, n_estimators=200; total time=   2.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=80, n_estimators=200; total time=   5.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=80, n_estimators=200; total time=   6.3s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=80, n_estimators=200; total time=   6.5s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.07, max_depth=110, n_estimators=1400; total time=  10.4s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.07, max_depth=110, n_estimators=1400; total time=  10.4s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.07, max_depth=40, n_estimators=400; total time=  10.8s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.07, max_depth=40, n_estimators=400; total time=  11.3s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.07, max_depth=110, n_estimators=1400; total time=  11.5s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.07, max_depth=40, n_estimators=400; total time=  12.0s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.16, max_depth=100, n_estimators=1000; total time=  16.9s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.16, max_depth=100, n_estimators=1000; total time=  17.1s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.16, max_depth=100, n_estimators=1000; total time=  18.3s\n",
            "[CV] END colsample_bytree=0.4, learning_rate=0.14, max_depth=110, n_estimators=1400; total time=  18.6s\n",
            "[CV] END colsample_bytree=0.4, learning_rate=0.14, max_depth=110, n_estimators=1400; total time=  18.7s\n",
            "[CV] END colsample_bytree=0.4, learning_rate=0.14, max_depth=110, n_estimators=1400; total time=  20.8s\n",
            "[CV] END colsample_bytree=0.2, learning_rate=0.09, max_depth=10, n_estimators=1200; total time=  12.2s\n",
            "[CV] END colsample_bytree=0.2, learning_rate=0.09, max_depth=10, n_estimators=1200; total time=  12.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=50, n_estimators=600; total time=  13.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=50, n_estimators=600; total time=  13.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=90, n_estimators=1000; total time=  24.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=90, n_estimators=1000; total time=  25.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=50, n_estimators=600; total time=  15.2s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.14, max_depth=60, n_estimators=1800; total time=  25.6s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.14, max_depth=60, n_estimators=1800; total time=  25.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=80, n_estimators=400; total time=  20.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=90, n_estimators=1000; total time=  28.1s\n",
            "[CV] END colsample_bytree=0.2, learning_rate=0.09, max_depth=10, n_estimators=1200; total time=  11.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=80, n_estimators=400; total time=  22.7s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.14, max_depth=60, n_estimators=1800; total time=  29.4s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.14, max_depth=50, n_estimators=2000; total time=  12.9s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.14, max_depth=50, n_estimators=2000; total time=  11.8s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.14, max_depth=50, n_estimators=2000; total time=  12.3s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.05, max_depth=30, n_estimators=1200; total time=  30.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.09, max_depth=10, n_estimators=1000; total time=  30.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=80, n_estimators=400; total time=  21.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.09, max_depth=90, n_estimators=800; total time=  29.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.09, max_depth=10, n_estimators=1000; total time=  31.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.14, max_depth=None, n_estimators=1400; total time=  30.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.14, max_depth=None, n_estimators=1400; total time=  30.0s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.05, max_depth=30, n_estimators=1200; total time=  32.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.14, max_depth=None, n_estimators=1400; total time=  31.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.09, max_depth=90, n_estimators=800; total time=  28.6s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.05, max_depth=30, n_estimators=1200; total time=  34.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.09, max_depth=10, n_estimators=1000; total time=  34.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.09, max_depth=90, n_estimators=800; total time=  32.0s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.2, max_depth=110, n_estimators=1000; total time=   6.2s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.07, max_depth=70, n_estimators=200; total time=   7.0s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.09, max_depth=40, n_estimators=400; total time=  10.1s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.07, max_depth=70, n_estimators=200; total time=   7.7s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.2, max_depth=110, n_estimators=1000; total time=   6.5s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.07, max_depth=70, n_estimators=200; total time=   7.0s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.09, max_depth=40, n_estimators=400; total time=  10.2s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.09, max_depth=40, n_estimators=400; total time=  11.9s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.2, max_depth=110, n_estimators=1000; total time=   7.2s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.05, max_depth=20, n_estimators=400; total time=  11.4s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.05, max_depth=20, n_estimators=400; total time=  12.4s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.05, max_depth=20, n_estimators=400; total time=  11.8s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.01, max_depth=40, n_estimators=800; total time=  15.8s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.01, max_depth=40, n_estimators=800; total time=  15.1s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.01, max_depth=40, n_estimators=800; total time=  16.5s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.18, max_depth=30, n_estimators=800; total time=  22.5s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.18, max_depth=30, n_estimators=800; total time=  21.1s\n",
            "[CV] END colsample_bytree=0.4, learning_rate=0.16, max_depth=None, n_estimators=1200; total time=  13.0s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.18, max_depth=30, n_estimators=800; total time=  20.8s\n",
            "[CV] END colsample_bytree=0.4, learning_rate=0.16, max_depth=None, n_estimators=1200; total time=  13.0s\n",
            "[CV] END colsample_bytree=0.4, learning_rate=0.16, max_depth=None, n_estimators=1200; total time=  13.4s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.18, max_depth=30, n_estimators=1200; total time=  13.5s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.18, max_depth=30, n_estimators=1200; total time=  14.5s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.18, max_depth=30, n_estimators=1200; total time=  15.3s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.16, max_depth=110, n_estimators=2000; total time=  29.9s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.16, max_depth=110, n_estimators=2000; total time=  29.9s\n",
            "[CV] END colsample_bytree=0.2, learning_rate=0.16, max_depth=10, n_estimators=1600; total time=  13.8s\n",
            "[CV] END colsample_bytree=0.2, learning_rate=0.16, max_depth=10, n_estimators=1600; total time=  13.8s\n",
            "[CV] END colsample_bytree=0.2, learning_rate=0.16, max_depth=10, n_estimators=1600; total time=  14.6s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.16, max_depth=110, n_estimators=2000; total time=  33.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.16, max_depth=40, n_estimators=1000; total time=  22.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.16, max_depth=40, n_estimators=1000; total time=  22.8s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.16, max_depth=40, n_estimators=1000; total time=  26.3s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.07, max_depth=40, n_estimators=1400; total time=  31.2s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.07, max_depth=40, n_estimators=1400; total time=  32.1s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.07, max_depth=40, n_estimators=1400; total time=  35.6s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.05, max_depth=110, n_estimators=1600; total time=  24.7s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.03, max_depth=40, n_estimators=1800; total time=  32.0s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.05, max_depth=110, n_estimators=1600; total time=  25.0s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.05, max_depth=110, n_estimators=1600; total time=  28.2s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.03, max_depth=70, n_estimators=2000; total time=  15.6s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.03, max_depth=70, n_estimators=2000; total time=  15.7s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.03, max_depth=40, n_estimators=1800; total time=  31.7s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.03, max_depth=70, n_estimators=2000; total time=  17.7s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.03, max_depth=40, n_estimators=1800; total time=  35.7s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.05, max_depth=100, n_estimators=1000; total time=  17.3s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.05, max_depth=100, n_estimators=1000; total time=  19.7s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.05, max_depth=100, n_estimators=1000; total time=  17.2s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.12, max_depth=10, n_estimators=1200; total time=  16.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.12, max_depth=50, n_estimators=2000; total time=  37.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.14, max_depth=50, n_estimators=1600; total time=  33.1s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.12, max_depth=10, n_estimators=1200; total time=  14.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.12, max_depth=50, n_estimators=2000; total time=  38.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=30, n_estimators=1400; total time=  42.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=30, n_estimators=1400; total time=  42.4s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.12, max_depth=10, n_estimators=1200; total time=  14.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.14, max_depth=50, n_estimators=1600; total time=  33.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.14, max_depth=50, n_estimators=1600; total time=  36.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.12, max_depth=50, n_estimators=2000; total time=  41.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=30, n_estimators=1400; total time=  46.6s\n",
            "[CV] END colsample_bytree=0.4, learning_rate=0.05, max_depth=60, n_estimators=200; total time=   4.9s\n",
            "[CV] END colsample_bytree=0.4, learning_rate=0.05, max_depth=60, n_estimators=200; total time=   5.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=60, n_estimators=1400; total time=  51.5s\n",
            "[CV] END colsample_bytree=0.4, learning_rate=0.05, max_depth=60, n_estimators=200; total time=   5.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=60, n_estimators=1400; total time=  50.3s\n",
            "[CV] END colsample_bytree=0.2, learning_rate=0.12, max_depth=100, n_estimators=2000; total time=  17.5s\n",
            "[CV] END colsample_bytree=0.2, learning_rate=0.12, max_depth=100, n_estimators=2000; total time=  17.8s\n",
            "[CV] END colsample_bytree=0.2, learning_rate=0.12, max_depth=100, n_estimators=2000; total time=  20.5s\n",
            "[CV] END colsample_bytree=0.4, learning_rate=0.18, max_depth=100, n_estimators=1800; total time=  23.9s\n",
            "[CV] END colsample_bytree=0.4, learning_rate=0.18, max_depth=100, n_estimators=1800; total time=  22.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=60, n_estimators=1400; total time=  56.6s\n",
            "[CV] END colsample_bytree=0.4, learning_rate=0.18, max_depth=100, n_estimators=1800; total time=  22.0s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.2, max_depth=80, n_estimators=1400; total time=  14.5s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.2, max_depth=80, n_estimators=1400; total time=  14.8s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.2, max_depth=80, n_estimators=1400; total time=  16.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.09, max_depth=60, n_estimators=1000; total time=  24.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=50, n_estimators=200; total time=  11.1s\n",
            "[CV] END colsample_bytree=0.2, learning_rate=0.01, max_depth=70, n_estimators=1600; total time=  23.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.09, max_depth=60, n_estimators=1000; total time=  25.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=50, n_estimators=200; total time=  12.0s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=50, n_estimators=200; total time=  11.4s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.09, max_depth=None, n_estimators=800; total time=  10.3s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.09, max_depth=None, n_estimators=800; total time=  10.7s\n",
            "[CV] END colsample_bytree=0.2, learning_rate=0.05, max_depth=90, n_estimators=1800; total time=  21.1s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.09, max_depth=None, n_estimators=800; total time=  10.6s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.07, max_depth=40, n_estimators=1800; total time=  51.3s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.09, max_depth=60, n_estimators=1000; total time=  27.8s\n",
            "[CV] END colsample_bytree=0.2, learning_rate=0.05, max_depth=90, n_estimators=1800; total time=  23.9s\n",
            "[CV] END colsample_bytree=0.2, learning_rate=0.01, max_depth=70, n_estimators=1600; total time=  27.1s\n",
            "[CV] END colsample_bytree=0.2, learning_rate=0.05, max_depth=90, n_estimators=1800; total time=  22.5s\n",
            "[CV] END colsample_bytree=0.2, learning_rate=0.01, max_depth=70, n_estimators=1600; total time=  26.3s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.07, max_depth=40, n_estimators=1800; total time=  54.4s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.09, max_depth=None, n_estimators=1000; total time=  23.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.09, max_depth=None, n_estimators=1000; total time=  24.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.09, max_depth=None, n_estimators=1000; total time=  25.1s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.07, max_depth=40, n_estimators=1800; total time=  48.9s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.18, max_depth=90, n_estimators=400; total time=   9.6s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.07, max_depth=40, n_estimators=600; total time=  22.2s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.18, max_depth=90, n_estimators=400; total time=  10.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.07, max_depth=40, n_estimators=600; total time=  24.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.07, max_depth=40, n_estimators=600; total time=  21.3s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.03, max_depth=100, n_estimators=600; total time=  12.6s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.03, max_depth=100, n_estimators=600; total time=  13.9s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.03, max_depth=100, n_estimators=600; total time=  13.3s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.16, max_depth=None, n_estimators=1200; total time=  24.0s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.16, max_depth=None, n_estimators=1200; total time=  26.2s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.16, max_depth=None, n_estimators=1200; total time=  25.2s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.12, max_depth=60, n_estimators=800; total time=  24.0s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.12, max_depth=60, n_estimators=800; total time=  24.8s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.12, max_depth=40, n_estimators=800; total time=  23.9s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.18, max_depth=90, n_estimators=400; total time=   9.7s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.12, max_depth=40, n_estimators=800; total time=  25.1s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.18, max_depth=70, n_estimators=1600; total time=  33.1s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.12, max_depth=60, n_estimators=800; total time=  26.5s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.18, max_depth=70, n_estimators=1600; total time=  33.5s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.12, max_depth=40, n_estimators=800; total time=  27.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=80, n_estimators=800; total time=  26.5s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.18, max_depth=70, n_estimators=1600; total time=  37.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=80, n_estimators=800; total time=  26.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=80, n_estimators=800; total time=  29.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.07, max_depth=80, n_estimators=1800; total time=  46.1s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=80, n_estimators=200; total time=   8.0s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=80, n_estimators=200; total time=   8.3s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.07, max_depth=80, n_estimators=1800; total time=  45.9s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.07, max_depth=80, n_estimators=1800; total time=  49.5s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=80, n_estimators=200; total time=   9.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=40, n_estimators=800; total time=  35.9s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.03, max_depth=None, n_estimators=1200; total time=  18.9s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.14, max_depth=60, n_estimators=1200; total time=   8.1s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.03, max_depth=None, n_estimators=1200; total time=  19.2s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.14, max_depth=60, n_estimators=1200; total time=   9.5s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.03, max_depth=None, n_estimators=1200; total time=  19.8s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.14, max_depth=60, n_estimators=1200; total time=   8.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=40, n_estimators=800; total time=  38.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=40, n_estimators=800; total time=  39.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=80, n_estimators=1200; total time=  24.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=80, n_estimators=1200; total time=  24.3s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.14, max_depth=40, n_estimators=600; total time=   9.0s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.14, max_depth=40, n_estimators=600; total time=  10.0s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.14, max_depth=40, n_estimators=600; total time=   9.2s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.12, max_depth=90, n_estimators=1400; total time=  25.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=80, n_estimators=1200; total time=  27.4s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.12, max_depth=90, n_estimators=1400; total time=  23.6s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.12, max_depth=90, n_estimators=1400; total time=  23.1s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.18, max_depth=70, n_estimators=400; total time=   3.4s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.18, max_depth=70, n_estimators=400; total time=   3.0s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.18, max_depth=70, n_estimators=400; total time=   3.1s\n",
            "[CV] END colsample_bytree=0.2, learning_rate=0.07, max_depth=30, n_estimators=1200; total time=  14.4s\n",
            "[CV] END colsample_bytree=0.2, learning_rate=0.07, max_depth=30, n_estimators=1200; total time=  14.6s\n",
            "[CV] END colsample_bytree=0.2, learning_rate=0.07, max_depth=30, n_estimators=1200; total time=  15.7s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=20, n_estimators=2000; total time=  28.0s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=20, n_estimators=2000; total time=  29.6s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.12, max_depth=80, n_estimators=800; total time=  11.6s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.12, max_depth=80, n_estimators=800; total time=  11.7s\n",
            "[CV] END colsample_bytree=0.3, learning_rate=0.12, max_depth=80, n_estimators=800; total time=  12.9s\n",
            "[CV] END colsample_bytree=0.4, learning_rate=0.05, max_depth=110, n_estimators=800; total time=  17.3s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=20, n_estimators=2000; total time=  32.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.18, max_depth=70, n_estimators=1400; total time=  24.9s\n",
            "[CV] END colsample_bytree=0.4, learning_rate=0.05, max_depth=110, n_estimators=800; total time=  18.3s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.18, max_depth=70, n_estimators=1400; total time=  25.6s\n",
            "[CV] END colsample_bytree=0.4, learning_rate=0.05, max_depth=110, n_estimators=800; total time=  20.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.18, max_depth=70, n_estimators=1400; total time=  27.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=None, n_estimators=1200; total time=  21.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=None, n_estimators=1200; total time=  21.4s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.2, max_depth=70, n_estimators=1800; total time=  34.9s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.03, max_depth=100, n_estimators=1400; total time=  59.7s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.03, max_depth=100, n_estimators=1400; total time= 1.0min\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.2, max_depth=70, n_estimators=1800; total time=  35.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.05, max_depth=None, n_estimators=1200; total time=  21.5s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.03, max_depth=50, n_estimators=400; total time=   2.9s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.03, max_depth=50, n_estimators=400; total time=   3.1s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.03, max_depth=50, n_estimators=400; total time=   3.1s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.2, max_depth=70, n_estimators=1800; total time=  39.0s\n",
            "[CV] END colsample_bytree=0.4, learning_rate=0.05, max_depth=10, n_estimators=800; total time=  14.6s\n",
            "[CV] END colsample_bytree=0.4, learning_rate=0.05, max_depth=10, n_estimators=800; total time=  14.0s\n",
            "[CV] END colsample_bytree=0.4, learning_rate=0.05, max_depth=10, n_estimators=800; total time=  14.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.03, max_depth=100, n_estimators=1400; total time= 1.1min\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.16, max_depth=20, n_estimators=800; total time=  24.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.16, max_depth=20, n_estimators=800; total time=  24.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.16, max_depth=20, n_estimators=800; total time=  25.7s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.16, max_depth=40, n_estimators=800; total time=  16.0s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.18, max_depth=40, n_estimators=1200; total time=   8.4s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.18, max_depth=40, n_estimators=1200; total time=   7.4s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.16, max_depth=40, n_estimators=800; total time=  18.0s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.16, max_depth=40, n_estimators=800; total time=  16.1s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.18, max_depth=40, n_estimators=1200; total time=   8.7s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.09, max_depth=70, n_estimators=1600; total time=  42.5s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.16, max_depth=40, n_estimators=1200; total time=  19.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=None, n_estimators=200; total time=   4.7s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.09, max_depth=70, n_estimators=1600; total time=  42.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=None, n_estimators=200; total time=   4.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=None, n_estimators=200; total time=   5.3s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.16, max_depth=40, n_estimators=1200; total time=  20.7s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.16, max_depth=40, n_estimators=1200; total time=  19.0s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.09, max_depth=70, n_estimators=1600; total time=  47.5s\n",
            "[CV] END colsample_bytree=0.4, learning_rate=0.12, max_depth=100, n_estimators=1600; total time=  22.0s\n",
            "[CV] END colsample_bytree=0.4, learning_rate=0.12, max_depth=100, n_estimators=1600; total time=  22.8s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=100, n_estimators=600; total time=  30.4s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=100, n_estimators=600; total time=  30.3s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.01, max_depth=100, n_estimators=600; total time=  31.8s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.18, max_depth=60, n_estimators=1400; total time=   9.1s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.18, max_depth=60, n_estimators=1400; total time=   8.4s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.18, max_depth=60, n_estimators=1400; total time=   8.6s\n",
            "[CV] END colsample_bytree=0.4, learning_rate=0.12, max_depth=100, n_estimators=1600; total time=  25.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.14, max_depth=70, n_estimators=2000; total time=  34.8s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.14, max_depth=70, n_estimators=2000; total time=  35.5s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.09, max_depth=100, n_estimators=800; total time=  26.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.14, max_depth=70, n_estimators=2000; total time=  38.8s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.09, max_depth=100, n_estimators=800; total time=  28.3s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=10, n_estimators=1200; total time=  46.5s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=10, n_estimators=1200; total time=  47.7s\n",
            "[CV] END colsample_bytree=0.9, learning_rate=0.09, max_depth=100, n_estimators=800; total time=  31.1s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.14, max_depth=20, n_estimators=1000; total time=  29.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.14, max_depth=20, n_estimators=1000; total time=  31.6s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.14, max_depth=20, n_estimators=1000; total time=  28.8s\n",
            "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=10, n_estimators=1200; total time=  49.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.12, max_depth=100, n_estimators=600; total time=  16.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.12, max_depth=100, n_estimators=600; total time=  16.4s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.16, max_depth=30, n_estimators=800; total time=  16.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.12, max_depth=100, n_estimators=600; total time=  18.2s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.16, max_depth=30, n_estimators=800; total time=  16.4s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.16, max_depth=40, n_estimators=1800; total time=  10.9s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.16, max_depth=40, n_estimators=1800; total time=  11.5s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.16, max_depth=30, n_estimators=800; total time=  17.5s\n",
            "[CV] END colsample_bytree=0.1, learning_rate=0.16, max_depth=40, n_estimators=1800; total time=  10.7s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.18, max_depth=110, n_estimators=1600; total time=  31.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.18, max_depth=110, n_estimators=1600; total time=  32.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.18, max_depth=110, n_estimators=1600; total time=  30.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.12, max_depth=None, n_estimators=1400; total time=  21.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.12, max_depth=None, n_estimators=1400; total time=  23.2s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.12, max_depth=None, n_estimators=1400; total time=  21.1s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.07, max_depth=70, n_estimators=2000; total time=  41.6s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.12, max_depth=70, n_estimators=2000; total time=  35.4s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.12, max_depth=70, n_estimators=2000; total time=  35.5s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.07, max_depth=70, n_estimators=2000; total time=  42.7s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.07, max_depth=70, n_estimators=2000; total time=  45.0s\n",
            "[CV] END colsample_bytree=0.7, learning_rate=0.12, max_depth=70, n_estimators=2000; total time=  38.6s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.07, max_depth=80, n_estimators=2000; total time=  36.9s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.07, max_depth=80, n_estimators=2000; total time=  39.4s\n",
            "[CV] END colsample_bytree=0.6, learning_rate=0.07, max_depth=80, n_estimators=2000; total time=  35.2s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.16, max_depth=30, n_estimators=1800; total time=  27.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.16, max_depth=30, n_estimators=1800; total time=  26.4s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.16, max_depth=30, n_estimators=1800; total time=  32.0s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=90, n_estimators=1600; total time=  37.0s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=10, n_estimators=2000; total time=  45.1s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=10, n_estimators=2000; total time=  46.5s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=90, n_estimators=1600; total time=  36.5s\n",
            "[CV] END colsample_bytree=0.8, learning_rate=0.03, max_depth=10, n_estimators=2000; total time=  49.7s\n",
            "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=90, n_estimators=1600; total time=  41.3s\n",
            "{'n_estimators': 200, 'max_depth': 80, 'learning_rate': 0.01, 'colsample_bytree': 0.7}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
        "max_depth.append(None)\n",
        "learning_rate=[round(float(x),2) for x in np.linspace(start=0.01, stop=0.2, num=10)]\n",
        "colsample_bytree =[round(float(x),2) for x in np.linspace(start=0.1, stop=1, num=10)]\n",
        "\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_depth': max_depth,\n",
        "               'learning_rate': learning_rate,\n",
        "               'colsample_bytree': colsample_bytree}\n",
        "xgbrModel = RandomizedSearchCV(estimator = xgbrModel, param_distributions=random_grid,\n",
        "                              n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
        "\n",
        "xgbrModel.fit(training_data, labels)\n",
        "print(xgbrModel.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFmH4_ic8l4e"
      },
      "source": [
        "# 預測與結果輸出\n",
        "  利用訓練好的模型對目標alert key預測報SAR的機率以及輸出為目標格式。\n",
        "  目標輸出筆數3850，其中public筆數為1845筆。\n",
        "  因上傳格式需要private跟public alert key皆考慮，直接從預測範本統計要預測的alert key，預測結果輸出為prediction.csv。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HHc2MF1S9EA"
      },
      "outputs": [],
      "source": [
        "# Do missing value imputation and one-hot encoding for testing data\n",
        "test_numerical_data = testing_data[:, numerical_index]\n",
        "test_non_numerical_data = testing_data[:, non_numerical_index]\n",
        "\n",
        "test_numerical_data = imp_median.transform(test_numerical_data)\n",
        "\n",
        "test_non_numerical_data = imp_most_frequent.transform(test_non_numerical_data)\n",
        "\n",
        "testing_data = np.concatenate((test_non_numerical_data, test_numerical_data), axis=1)\n",
        "testing_data = onehotencorder.transform(testing_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apza1dJrXoe3",
        "outputId": "111f7c7e-4ef9-4ab0-a9f1-4a9597ffd9f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3850\n"
          ]
        }
      ],
      "source": [
        "# Read csv of all alert keys need to be predicted\n",
        "public_private_test_csv = os.path.join(data_dir, '預測的案件名單及提交檔案範例.csv')\n",
        "df_public_private_test = pd.read_csv(public_private_test_csv)\n",
        "\n",
        "# Predict probability\n",
        "predicted = []\n",
        "for i, _x in enumerate(xgbrModel.predict_proba(testing_data)):\n",
        "    predicted.append([testing_alert_key[i], _x[1]])\n",
        "predicted = sorted(predicted, key= lambda s: s[1])\n",
        "\n",
        "# 考慮private alert key部分，滿足上傳條件\n",
        "public_private_alert_key = df_public_private_test['alert_key'].values\n",
        "print(len(public_private_alert_key))\n",
        "\n",
        "# For alert key not in public, add zeros\n",
        "for key in public_private_alert_key:\n",
        "    if key not in testing_alert_key:\n",
        "        predicted.append([key, 0])\n",
        "\n",
        "predict_alert_key, predict_probability = [], []\n",
        "for key, prob in predicted:\n",
        "    predict_alert_key.append(key)\n",
        "    predict_probability.append(prob)\n",
        "\n",
        "df_predicted = pd.DataFrame({\n",
        "    \"alert_key\": predict_alert_key,\n",
        "    \"probability\": predict_probability\n",
        "})\n",
        "\n",
        "df_predicted.to_csv('result.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result:\n",
            "Accuracy Score: 99.40379403794037%\n",
            "Precision Score: 0.00%\n",
            "Recall Score: 0.00%\n",
            "F1 score: 0.00%\n",
            "Confusion Matrix:\n",
            " [[1834    0]\n",
            " [  11    0]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/yslin/anaconda3/envs/lab/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "def score(model, x, y):\n",
        "    pred = model.predict(x)\n",
        "    print('Result:')\n",
        "    print(f\"Accuracy Score: {accuracy_score(y, pred)*100:}%\")\n",
        "    print(f\"Precision Score: {precision_score(y, pred)*100:.2f}%\")\n",
        "    print(f\"Recall Score: {recall_score(y, pred)*100:.2f}%\")\n",
        "    print(f\"F1 score: {f1_score(y, pred)*100:.2f}%\")\n",
        "    print(f\"Confusion Matrix:\\n {confusion_matrix(y, pred)}\")\n",
        "\n",
        "def read_public_answer(data_dir):\n",
        "    y_csv = os.path.join(data_dir, '24_ESun_public_y_answer.csv')\n",
        "    df_y = pd.read_csv(y_csv)\n",
        "    return df_y.iloc[:, 1]\n",
        "\n",
        "public_label = read_public_answer('./data')\n",
        "score(xgbrModel, testing_data[:1845], public_label)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1845, 2)\n",
            "      alert_key  probability\n",
            "0        353220     0.070268\n",
            "1        360572     0.070268\n",
            "2        361565     0.070268\n",
            "3        367723     0.070268\n",
            "4        372107     0.070268\n",
            "...         ...          ...\n",
            "3845     375610     0.261708\n",
            "3846     372351     0.262284\n",
            "3847     373827     0.263037\n",
            "3848     378054     0.267724\n",
            "3849     376670     0.335035\n",
            "\n",
            "[3850 rows x 2 columns]\n",
            "11\n",
            "355152 0 1688\n",
            "361617 1 1812\n",
            "359668 2 2088\n",
            "358453 3 2098\n",
            "356602 4 2388\n",
            "355724 5 2418\n",
            "363033 6 2745\n",
            "355091 7 2895\n",
            "354939 8 3148\n",
            "363320 9 3248\n",
            "found =  10\n",
            "catch =  3249\n",
            "SAR Score =  0.0030778701138811943\n"
          ]
        }
      ],
      "source": [
        "def getResultFromAlertKey(alert_key, df_truth, found, catch):\n",
        "  for i , answer_key in enumerate(df_truth.iloc[:, 0]):\n",
        "    answer_val = df_truth.iloc[i][1]\n",
        "    if alert_key == int(answer_key) and answer_val == 1:\n",
        "      print(alert_key, found, catch)\n",
        "      return 1\n",
        "  return 0\n",
        "\n",
        "# pos = []\n",
        "# def getInfoFromAlertKey(alert_key, ):\n",
        "#   df = df_training_with_key\n",
        "#   for i , answer_key in enumerate(df.iloc[:, 0]):\n",
        "#     if alert_key == int(answer_key):\n",
        "#       # print(df.iloc[i, :])\n",
        "#       pos.append(df.iloc[i, :].values)\n",
        "#       break\n",
        "#   return\n",
        "\n",
        "def sar():\n",
        "  df_truth = pd.read_csv('data/24_ESun_public_y_answer.csv')\n",
        "  df_prediction = pd.read_csv('prediction.csv')\n",
        "  \n",
        "  print(df_truth.shape)\n",
        "  print(df_prediction)\n",
        "\n",
        "  N = 0\n",
        "  for i , val in enumerate(df_truth.iloc[:, 1]):\n",
        "    if val == 1:\n",
        "      N += 1 \n",
        "  print(N)\n",
        "\n",
        "  found = 0\n",
        "  catch = 0\n",
        "  for i , item in enumerate(df_prediction['alert_key']):\n",
        "    # getInfoFromAlertKey(int(item), df_truth)\n",
        "    found += getResultFromAlertKey(int(item), df_truth, found, catch)\n",
        "    catch += 1\n",
        "    if found == N-1:\n",
        "      break\n",
        "  print('found = ', found)\n",
        "  print('catch = ', catch)\n",
        "  print('SAR Score = ', found/catch)\n",
        "\n",
        "sar()\n",
        "# df_pos = pd.DataFrame(pos, columns = df_training_with_key.columns.tolist())\n",
        "# print(df_pos)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "714d180b42561b4f2601021ba5b5e711246367afcd25c29c42f70e4c0dbeeaef"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
